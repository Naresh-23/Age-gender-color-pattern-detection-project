5.1 AGE AND GENDER CODE
import cv2
import math
import argparse # optional to use, used to tame the input from the command
prompt
def highlightFace(net, frame, conf_threshold=0.7):
 frameOpencvDnn=frame.copy()
 frameHeight=frameOpencvDnn.shape[0]
 frameWidth=frameOpencvDnn.shape[1]
 blob=cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104,
117, 123], True, False)
 net.setInput(blob) 
- 106 -
 detections=net.forward()
 faceBoxes=[]
 for i in range(detections.shape[2]):
 confidence=detections[0,0,i,2]
 if confidence>conf_threshold:
 x1=int(detections[0,0,i,3]*frameWidth)
 y1=int(detections[0,0,i,4]*frameHeight)
 x2=int(detections[0,0,i,5]*frameWidth)
 y2=int(detections[0,0,i,6]*frameHeight)
 faceBoxes.append([x1,y1,x2,y2])
 cv2.rectangle(frameOpencvDnn, (x1,y1), (x2,y2), (0,255,0),
int(round(frameHeight/150)), 8)
 return frameOpencvDnn,faceBoxes
parser=argparse.ArgumentParser()
parser.add_argument('--image')
args=parser.parse_args()
#img="female30to35.jpg"
faceProto="opencv_face_detector.pbtxt"
faceModel="opencv_face_detector_uint8.pb"
ageProto="age_deploy.prototxt"
ageModel="age_net.caffemodel"
genderProto="gender_deploy.prototxt"
genderModel="gender_net.caffemodel" 
- 107 -
MODEL_MEAN_VALUES=(78.4263377603,87.7689143744, 114.895847746)
ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-
100)']
genderList=['Male','Female']
faceNet=cv2.dnn.readNet(faceModel,faceProto)
ageNet=cv2.dnn.readNet(ageModel,ageProto)
genderNet=cv2.dnn.readNet(genderModel,genderProto)
video=cv2.VideoCapture(0)
padding=20
while cv2.waitKey(1)<0:
 hasFrame,frame=video.read()
 if not hasFrame:
 cv2.waitKey()
 bream
 resultImg,faceBoxes=highlightFace(faceNet,frame)
 if not faceBoxes:
 print("No face detected")
 for faceBox in faceBoxes:
 face=frame[max(0,faceBox[1]-padding):
 min(faceBox[3]+padding,frame.shape[0]-1),max(0,faceBox[0]-padding)
 :min(faceBox[2]+padding, frame.shape[1]-1)] 
- 108 -

blob=cv2.dnn.blobFromImage(face,1.0,(227,227),MODEL_MEAN_VALUES,
swapRB=False)
 genderNet.setInput(blob)
 genderPreds=genderNet.forward()
 gender=genderList[genderPreds[0].argmax()]
 print(f'Gender: {gender}')
 ageNet.setInput(blob)
 agePreds=ageNet.forward()
 age=ageList[agePreds[0].argmax()]
 print(f'Age: {age[1:-1]} years')
 cv2.putText(resultImg, f'{gender}, {age}', (faceBox[0], faceBox[1]-10),
cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,255), 2, cv2.LINE_AA)
 cv2.imshow("Detecting age and gender", resultImg)
5.2 CONOR DETERMINATION CODE
# import the necessary pacmages
import numpy as np
import pandas as pd
import cv2
x=0
y=0
w=0
h=0 
- 109 -
index=["color","color_name","hex","R","G","B"]
csv = pd.read_csv('colors.csv', names=index, header=None)
def getColorName(R,G,B):
 minimum = 10000
 for i in range(len(csv)):
 d = abs(R- int(csv.loc[i,"R"])) + abs(G- int(csv.loc[i,"G"]))+ abs(Bint(csv.loc[i,"B"]))
 if(d<=minimum):
 minimum = d
 cname = csv.loc[i,"color_name"]
 return cname
# initialize the HOG descriptor/person detector
#def pdet():
hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
cv2.startWindowThread()
# open webcam video stream
cap = cv2.VideoCapture(0)
# the output will be written to output.avi
out = cv2.VideoWriter(
 'output.avi',
 cv2.VideoWriter_fourcc(*'MJPG'), 15., (640,480)) 
- 110 -
while(True):
 # Capture frame-by-frame ret,
frame = cap.read()
 # resizing for faster detection
 frame = cv2.resize(frame, (640, 480))
 # using a greyscale picture, also for faster detection
 gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
 # detect people in the image
 # returns the bounding boxes for the detected objects
 boxes, weights = hog.detectMultiScale(frame, winStride=(8,8) )
 print(boxes)
 boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])
 for (xA, yA, xB, yB) in boxes:
 print ("xA", xA)
 print ("yA", yA)
 print ("x+wA", xB)
 print ("y+hA", yB)
 # display the detected boxes in the colour picture
 cv2.rectangle(frame, (xA, yA), (xB, yB), (0, 255, 0), 2)
 roi=frame[yA:yB,xA:xB]
 cv2.imwrite("boxes.png",roi) 
- 111 -

 #Selecting the midpoint in the video
 point = ((yB - yA) // 2 + yA, (xA - xB) // 2 + xA)
 b, g, r = frame[point]
 text = getColorName(r,g,b) + ' R='+ str(r) + ' G='+ str(g) + ' B='+ str(b)
 #cv2.putText(img,text,start,font(0-7),fontScale,color,thicmness,lineType )
 cv2.putText(frame, text,(50,50),2,0.8,(255,255,255),2,cv2.LINE_AA)

 # Write the output video
 out.write(frame.astype('uint8'))
 # Display the resulting frame
 cv2.imshow('frame',frame)
 if cv2.waitKey(1) & 0xFF == ord('q'):
 bream
# When everything done, release the capture
cap.release()
# and release the output
out.release()
# finally, close the window
cv2.destroyAllWindows()
cv2.waitKey(1)
# return frame 
- 112 -
4.3 PATTERN DETERMINATION CODE
5.3.1 TRAINING CODE
5.3.1.1 MAIN CODE
# -*- coding: utf-8 -*-
"""
Created on Thu Mar 19 17:21:20 2020
@author: User
"""
# set the matplotlib bacmend so figures can be saved in the bacmground
import matplotlib
matplotlib.use("Agg")
# import the necessary pacmages
from pyimagesearch.minivggnet import MiniVGGNet
from smlearn.metrics import classification_report
from meras.optimizers import SGD
from meras.datasets import fashion_mnist
from meras.utils import np_utils
from meras import bacmend as K
from imutils import build_montages
import matplotlib.pyplot as plt
import numpy as np
import cv2 
- 113 -
# initialize the number of epochs to train for, base learning rate,
# and batch size
NUM_EPOCHS = 30
INIT_LR = 1e-2
BS = 32
# grab the Fashion MNIST dataset (if this is your first time running
# this the dataset will be automatically downloaded)
print("[INFO] loading Fashion MNIST...")
((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()
# if we are using "channels first" ordering, then reshape the design
# matrix such that the matrix is:
# num_samples x depth x rows x columns
if K.image_data_format() == "channels_first":
 trainX = trainX.reshape((trainX.shape[0], 1, 28, 28))
 testX = testX.reshape((testX.shape[0], 1, 28, 28))

# otherwise, we are using "channels last" ordering, so the design
# matrix shape should be: num_samples x rows x columns x depth
else:
 trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
 testX = testX.reshape((testX.shape[0], 28, 28, 1))
# scale data to the range of [0, 1] 
- 114 -
trainX = trainX.astype("float32") / 255.0
testX = testX.astype("float32") / 255.0
# one-hot encode the training and testing labels
trainY = np_utils.to_categorical(trainY, 10)
testY = np_utils.to_categorical(testY, 10)
# initialize the label names
labelNames = ["top", "trouser", "pullover", "dress", "coat",
 "sandal", "shirt", "sneamer", "bag", "anmle boot"]
# initialize the optimizer and model
print("[INFO] compiling model...")
opt = SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / NUM_EPOCHS)
model = MiniVGGNet.build(width=28, height=28, depth=1, classes=10)
model.compile(loss="categorical_crossentropy", optimizer=opt,
 metrics=["accuracy"])
# train the networm
print("[INFO] training model...")
H = model.fit(trainX, trainY,
 validation_data=(testX, testY),
 batch_size=BS, epochs=NUM_EPOCHS)
model.save("pattern_classifier_3.h5")
preds = model.predict(testX)
# show a nicely formatted classification report 
- 115 -
print("[INFO] evaluating networm...")
print(classification_report(testY.argmax(axis=1), preds.argmax(axis=1),
 target_names=labelNames))
#model.save("pattern_classifier_3.h5")
# plot the training loss and accuracy
N = NUM_EPOCHS
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, N), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, N), H.history["acc"], label="train_acc")
plt.plot(np.arange(0, N), H.history["val_acc"], label="val_acc")
plt.title("Training Loss and Accuracy on Dataset")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="lower left")
plt.savefig("plot.png")
# initialize our list of output images
images = []
# randomly select a few testing fashion items
for i in np.random.choice(np.arange(0, len(testY)), size=(16,)): 
- 116 -
 # classify the clothing
 probs = model.predict(testX[np.newaxis, i])
 prediction = probs.argmax(axis=1)
 label = labelNames[prediction[0]]

 # extract the image from the testData if using "channels_first"
 # ordering
 if K.image_data_format() == "channels_first":
 image = (testX[i][0] * 255).astype("uint8")

 # otherwise we are using "channels_last" ordering
 else:
 image = (testX[i] * 255).astype("uint8")
# initialize the text label color as green (correct)
 color = (0, 255, 0)
 # otherwise, the class label prediction is incorrect
 if prediction[0] != np.argmax(testY[i]):
 color = (0, 0, 255)

 # merge the channels into one image and resize the image from
 # 28x28 to 96x96 so we can better see it and then draw the 
- 117 -
 # predicted label on the image
 image = cv2.merge([image] * 3)
 image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)
 cv2.putText(image, label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX,
0.75, color, 2)
 # add the image to our list of output images
 images.append(image)
# construct the montage for the images
montage = build_montages(images, (96, 96), (4, 4))[0]
# show the output montage
cv2.imshow("Fashion MNIST", montage)
cv2.waitKey(0) 
